# EU AI Act (Regulation 2024/1689) Summary

**Full Title:** Regulation (EU) 2024/1689 of the European Parliament and of the Council laying down harmonised rules on artificial intelligence  
**Published:** July 12, 2024 (Official Journal)  
**Entry into Force:** August 1, 2024  
**Type:** Binding EU regulation  
**Status:** World's first comprehensive AI regulation

There is also an AI-powered summary video on YouTube: [EU AI Act Summary](https://youtu.be/QTUWY_SRLx0)

---

## Document Status

✅ **This summary is based on the actual EU AI Act regulation** (downloaded PDF available in this repository).

**Source:**
- Regulation (EU) 2024/1689
- Official Journal of the European Union L, 2024/1689
- Available at: https://eur-lex.europa.eu/legal-content/EN/TXT/PDF/?uri=OJ:L_202401689

---

## Purpose and Scope

The EU AI Act establishes **harmonized rules** for the development, placing on the market, and use of artificial intelligence systems in the European Union.

### How to Use This Summary
- **Leaders and decision-makers:** Skim the Purpose and Scope, Risk-Based Approach, and Timeline sections to understand where your use cases are likely to sit.
- **Risk, legal, and compliance roles:** Focus on High-Risk Requirements, GPAI rules, Stakeholder Obligations, and the Compliance Strategy and Key Considerations sections.
- **For AIOF/AIRMF work:** Use the AIOF and AIRMF alignment sections as a bridge from the regulation into your own opportunity and risk management frameworks.

### Key Objectives:
- Ensure AI systems placed on EU market are safe and respect fundamental rights
- Provide legal certainty for businesses and innovation
- Facilitate development of single market for lawful, safe, trustworthy AI
- Prevent fragmentation of internal market
- Establish governance and enforcement framework
- Support innovation through regulatory sandboxes

### Territorial Scope:
- **Providers** placing AI systems on EU market or putting into service in EU
- **Deployers** of AI systems located in EU
- **Providers and deployers** outside EU where output used in EU
- Applies regardless of where provider/deployer is established

### Material Scope:
- AI systems as defined in the regulation
- Certain prohibited AI practices
- High-risk AI systems
- General-purpose AI models
- Transparency obligations

---

## Risk-Based Approach

The EU AI Act uses a **four-tier risk classification** system:

### 1. Unacceptable Risk (Prohibited AI Practices)
**Article 5 - Prohibited Artificial Intelligence Practices**

**Completely banned:**
- Subliminal manipulation causing harm
- Exploitation of vulnerabilities (age, disability, socio-economic situation)
- Social scoring by public authorities
- Real-time remote biometric identification in publicly accessible spaces (with limited exceptions for law enforcement)
- Biometric categorization to infer sensitive attributes
- Emotion recognition in workplace and education (with exceptions)
- Untargeted scraping of facial images for facial recognition databases
- Inferring emotions in law enforcement (with exceptions)

**Rationale:** These practices violate fundamental rights and EU values.

---

### 2. High Risk
**Articles 6-51 - High-Risk AI Systems**

**Definition:** AI systems that pose significant risks to health, safety, or fundamental rights.

**Categories (Annex III):**

**1. Biometrics**
- Remote biometric identification
- Biometric categorization
- Emotion recognition (limited contexts)

**2. Critical Infrastructure**
- Management and operation of critical infrastructure (transport, water, gas, electricity, etc.)

**3. Education and Vocational Training**
- Determining access to educational institutions
- Assessing students
- Detecting prohibited behavior during tests

**4. Employment**
- Recruitment and selection
- Promotion and termination decisions
- Task allocation
- Monitoring and evaluation of performance

**5. Essential Private and Public Services**
- Creditworthiness assessment
- Credit scoring
- Emergency response dispatch
- Risk assessment for health/life insurance

**6. Law Enforcement**
- Individual risk assessment for criminal offenses
- Polygraphs and similar tools
- Evaluation of reliability of evidence
- Crime analytics (predicting occurrence or reoccurrence)
- Profiling in criminal investigations

**7. Migration, Asylum, and Border Control**
- Polygraphs and similar tools
- Risk assessment for irregular immigration
- Verification of authenticity of travel documents
- Examination of asylum applications

**8. Administration of Justice and Democratic Processes**
- Assisting judicial authorities in legal research and interpretation
- Influencing election outcomes or voter behavior

**Requirements for High-Risk AI Systems:**
- Risk management system (Article 9)
- Data governance and management (Article 10)
- Technical documentation (Article 11, Annex IV)
- Record-keeping and logging (Article 12)
- Transparency and information to deployers (Article 13)
- Human oversight (Article 14)
- Accuracy, robustness, cybersecurity (Article 15)
- Quality management system (Article 17)
- Conformity assessment (Articles 43-44)
- Registration in EU database (Article 49)
- Post-market monitoring (Article 72)

---

### 3. Limited Risk (Transparency Obligations)
**Articles 50-51 - Transparency Obligations**

**AI systems requiring transparency:**
- AI systems intended to interact with natural persons
- Emotion recognition systems
- Biometric categorization systems
- AI systems generating or manipulating content (deepfakes)

**Requirements:**
- Inform natural persons they are interacting with AI
- Disclose AI-generated or manipulated content
- Mark synthetic content appropriately
- Enable detection of AI-generated content

---

### 4. Minimal Risk
**No specific obligations beyond general law**

**Examples:**
- AI-enabled video games
- Spam filters
- Inventory management systems
- Most AI applications

**Voluntary codes of conduct encouraged** (Article 95)

---

## General-Purpose AI Models (GPAI)

**Chapter V (Articles 51-56) - Special Rules for GPAI Models**

### Definition:
AI models trained on broad data, capable of serving various purposes, including tasks not anticipated at development.

### Two Tiers:

**1. All GPAI Models (Article 53)**
**Requirements:**
- Technical documentation
- Information and documentation for downstream providers
- Copyright compliance policy
- Publicly available summary of training data

**2. GPAI Models with Systemic Risk (Article 55)**
**Additional requirements when:**
- High-impact capabilities (e.g., training compute > 10^25 FLOPs)
- Significant impact on EU market
- Designated by Commission

**Additional obligations:**
- Model evaluation (adversarial testing, red-teaming)
- Risk assessment and mitigation
- Tracking and reporting serious incidents
- Cybersecurity protection
- Energy efficiency reporting

---

## Key Requirements by Stakeholder

### Providers (Developers/Manufacturers)

**For High-Risk AI Systems:**
- Establish risk management system
- Ensure data quality and governance
- Create technical documentation
- Design for transparency and human oversight
- Ensure accuracy, robustness, cybersecurity
- Implement quality management system
- Conduct conformity assessment
- Affix CE marking
- Register in EU database
- Implement post-market monitoring
- Report serious incidents

**For GPAI Models:**
- Provide technical documentation
- Ensure copyright compliance
- Publish training data summary
- (If systemic risk) Conduct evaluations, assess risks, report incidents

### Deployers (Users of AI Systems)

**For High-Risk AI Systems:**
- Use in accordance with instructions
- Assign human oversight
- Monitor operation
- Keep logs
- Report serious incidents
- Conduct fundamental rights impact assessment (if public authority or certain sectors)
- Inform affected persons of their rights

**For All AI Systems:**
- Comply with transparency obligations
- Ensure lawful use
- Respect fundamental rights

### Importers and Distributors

**Responsibilities:**
- Verify provider compliance
- Ensure proper labeling and documentation
- Cooperate with authorities
- Report non-compliance

---

## Governance Structure

### EU Level:

**European Artificial Intelligence Board (Article 65)**
- Composed of national authorities
- Advises Commission
- Facilitates cooperation
- Issues recommendations and opinions

**AI Office (within Commission)**
- Supervises GPAI models
- Coordinates enforcement
- Supports innovation
- Manages EU database

### National Level:

**National Competent Authorities (Article 70)**
- Designated by each Member State
- Market surveillance
- Enforcement
- Cooperation with other authorities

**Notified Bodies (Article 33)**
- Conformity assessment for high-risk AI
- Accredited and designated by Member States

---

## Innovation Support

### AI Regulatory Sandboxes (Articles 53-55 - renumbered to 57-60 in final)

**Purpose:**
- Facilitate development and testing of innovative AI
- Provide controlled environment
- Reduce compliance uncertainty
- Support SMEs and startups

**Features:**
- Established by national authorities
- Supervised testing under real-world conditions
- Regulatory guidance and support
- Safeguards for participants and affected persons
- Time-limited participation

**Benefits:**
- Test AI before full market deployment
- Clarify regulatory requirements
- Reduce time-to-market
- Build evidence for compliance

### Support for SMEs and Startups (Article 57 - renumbered)

**Measures:**
- Priority access to sandboxes
- Reduced fees for conformity assessment
- Guidance and support from authorities
- Simplified compliance pathways (where appropriate)

---

## Penalties and Enforcement

**Article 99 - Administrative Fines**

**Maximum fines:**
- **€35 million or 7% of global annual turnover** (whichever higher) for:
  - Prohibited AI practices (Article 5)
  - Non-compliance with GPAI model obligations
- **€15 million or 3% of global annual turnover** for:
  - Non-compliance with high-risk AI requirements
  - Non-compliance with obligations of other operators
- **€7.5 million or 1.5% of global annual turnover** for:
  - Supplying incorrect, incomplete, or misleading information to authorities

**Factors considered:**
- Nature, gravity, duration of infringement
- Intentional or negligent character
- Actions taken to mitigate damage
- Degree of cooperation with authorities
- Previous infringements
- Financial benefits gained or losses avoided
- SME status

**Other enforcement measures:**
- Orders to bring AI system into compliance
- Withdrawal of AI system from market
- Recall of AI system
- Restriction or prohibition of placing on market
- Temporary or permanent ban from market

---

## Timeline and Phased Implementation

**Entry into force:** August 1, 2024

**Phased application:**

| Date | Provisions Applicable |
|------|----------------------|
| **February 2, 2025** (6 months) | Prohibited AI practices (Article 5) |
| **August 2, 2025** (12 months) | GPAI model obligations, governance, penalties |
| **August 2, 2026** (24 months) | Obligations for high-risk AI systems (with exceptions) |
| **August 2, 2027** (36 months) | Obligations for high-risk AI systems in products covered by existing EU legislation |
| **August 2, 2030** (72 months) | Obligations for certain high-risk AI systems already in use |

**Grandfathering:**
- AI systems lawfully placed on market before August 2, 2026 may continue if no significant changes
- Must comply by August 2, 2030 at latest

---

## Key Definitions

### AI System (Article 3(1))
A machine-based system that is designed to operate with varying levels of autonomy and that may exhibit adaptiveness after deployment, and that, for explicit or implicit objectives, infers, from the input it receives, how to generate outputs such as predictions, content, recommendations, or decisions that can influence physical or virtual environments.

### High-Risk AI System
AI system listed in Annex III or that is a safety component of a product covered by EU harmonization legislation listed in Annex I.

### Provider
A natural or legal person, public authority, agency, or other body that develops an AI system or a general-purpose AI model or that has an AI system or a general-purpose AI model developed and places it on the market or puts the AI system into service under its own name or trademark, whether for payment or free of charge.

### Deployer
A natural or legal person, public authority, agency, or other body using an AI system under its authority except where the AI system is used in the course of a personal non-professional activity.

### General-Purpose AI Model
An AI model, including where such an AI model is trained with a large amount of data using self-supervision at scale, that displays significant generality and is capable of competently performing a wide range of distinct tasks regardless of the way the model is placed on the market and that can be integrated into a variety of downstream systems or applications.

### Serious Incident
Any incident or malfunctioning of an AI system that directly or indirectly leads to death, serious damage to health, serious and irreversible disruption of critical infrastructure, breaches of fundamental rights, or serious damage to property or the environment.

---

## Alignment with AIOF

### Opportunity Discovery Context:

**Article 57 (Innovation Support):**
- Sandboxes enable exploration of AI opportunities
- Reduced regulatory uncertainty
- Support for testing innovative use cases
- **Directly supports AIOF Phase 2: Opportunity Identification**

**Risk Classification:**
- Understanding risk tier informs opportunity feasibility
- Low/minimal risk opportunities easier to pursue
- High-risk opportunities require more investment
- **Informs AIOF Phase 3: Opportunity Evaluation (Regulatory Fit dimension)**

**Transparency Obligations:**
- Clear requirements for certain AI types
- Enables informed opportunity assessment
- **Supports AIOF Phase 3: Opportunity Evaluation**

### AIOF-EU AI Act Mapping:

| AIOF Phase | EU AI Act Relevance |
|------------|---------------------|
| **Context Establishment** | Understanding regulatory landscape, risk tiers |
| **Opportunity Identification** | Sandboxes, innovation support, permissible use cases |
| **Opportunity Evaluation** | Regulatory Fit scoring (risk tier, compliance requirements) |
| **Prioritization** | Balance value with compliance complexity |
| **Governance** | Innovation governance policy aligned with EU AI Act |

---

## Alignment with AIRMF

### Risk Management Context:

**Prohibited Practices (Article 5):**
- Absolute red lines for AI systems
- Must be identified and avoided
- **AIRMF must screen for prohibited practices**

**High-Risk Requirements (Articles 6-51):**
- Detailed compliance obligations
- Risk management system required
- **AIRMF implements these requirements**

**Conformity Assessment:**
- Third-party assessment for certain high-risk systems
- Internal assessment for others
- **AIRMF prepares for conformity assessment**

**Post-Market Monitoring:**
- Ongoing surveillance required
- Incident reporting obligations
- **AIRMF includes monitoring procedures**

### AIRMF-EU AI Act Mapping:

| AIRMF Component | EU AI Act Requirement |
|-----------------|----------------------|
| **Risk Assessment** | Article 9 (Risk management system) |
| **Data Governance** | Article 10 (Data governance) |
| **Technical Documentation** | Article 11, Annex IV |
| **Logging and Monitoring** | Article 12 (Record-keeping), Article 72 (Post-market monitoring) |
| **Human Oversight** | Article 14 (Human oversight) |
| **Accuracy and Robustness** | Article 15 (Accuracy, robustness, cybersecurity) |
| **Quality Management** | Article 17 (Quality management system) |
| **Incident Response** | Article 73 (Reporting of serious incidents) |

---

## Compliance Strategy

### Step 1: Determine Applicability
- Is your AI system covered by the regulation?
- Are you a provider, deployer, importer, or distributor?
- Does the AI system operate in or affect the EU?

### Step 2: Classify Risk Level
- Is it a prohibited practice? → Do not develop/deploy
- Is it high-risk (Annex III)? → Full compliance required
- Does it require transparency? → Disclosure obligations
- Is it minimal risk? → Voluntary best practices

### Step 3: Understand Requirements
- Review specific obligations for your role and risk level
- Identify applicable articles and annexes
- Determine timeline for compliance

### Step 4: Implement Compliance Program
- Establish governance structure
- Develop required documentation
- Implement technical and organizational measures
- Train staff on obligations

### Step 5: Conformity Assessment (if high-risk)
- Prepare technical documentation
- Conduct internal or third-party assessment
- Affix CE marking (if compliant)
- Register in EU database

### Step 6: Ongoing Compliance
- Monitor AI system performance
- Maintain documentation
- Report serious incidents
- Update as requirements evolve

---

## Relationship to Other Standards

| Standard/Framework | Relationship to EU AI Act |
|--------------------|---------------------------|
| **ISO/IEC 42001** | AI management system can support compliance with governance and process requirements |
| **ISO/IEC 27001** | Information security controls support Article 15 (cybersecurity) |
| **ISO 31000** | Risk management principles align with Article 9 (risk management system) |
| **NIST AI RMF** | Trustworthy AI characteristics align with EU AI Act objectives; can inform compliance; unlike the voluntary NIST AI RMF, the EU AI Act is a **binding regulation** for any organization placing AI systems on the EU market or using them in the EU |
| **Harmonized Standards** | European standards (EN) provide presumption of conformity with EU AI Act requirements |

**Note:** EU Commission will develop harmonized standards that provide presumption of conformity. Organizations can also demonstrate compliance through other means.

---

## Key Considerations for Organizations

### For AI Providers:
- Understand if your AI systems are high-risk
- Implement required risk management and quality systems
- Prepare comprehensive technical documentation
- Plan for conformity assessment
- Budget for compliance costs
- Consider using sandboxes for innovative systems

### For AI Deployers:
- Verify provider compliance before procurement
- Understand your obligations (human oversight, monitoring, logging)
- Conduct fundamental rights impact assessments (if applicable)
- Train staff on proper use
- Establish incident reporting procedures

### For Organizations Using AI:
- Audit existing AI systems for compliance
- Prioritize high-risk systems for compliance efforts
- Engage legal and technical experts
- Monitor regulatory developments (harmonized standards, guidance)
- Consider certification to ISO 42001 to demonstrate good practices

### For Innovators and Startups:
- Leverage regulatory sandboxes
- Seek guidance from national authorities
- Design for compliance from the start
- Consider partnerships with established players
- Stay informed on SME support measures

---

## Resources and Guidance

### Official Resources:
- **EU AI Act text:** https://eur-lex.europa.eu/eli/reg/2024/1689/oj
- **European Commission AI page:** https://digital-strategy.ec.europa.eu/en/policies/regulatory-framework-ai
- **AI Office:** https://digital-strategy.ec.europa.eu/en/policies/ai-office

### National Authorities:
- Each EU Member State will designate competent authorities
- Contact national authority for guidance and sandbox access

### Harmonized Standards:
- To be developed by European standardization organizations (CEN, CENELEC, ETSI)
- Will provide presumption of conformity
- Monitor for publication

---

## Version History

| Version | Date | Changes |
|---------|------|---------|
| 0.1 (Summary) | Nov 2025 | Summary document created |

**Note:** This summary is based on Regulation (EU) 2024/1689 as published in July 2024. The EU Commission will issue implementing acts, delegated acts, and guidance. Harmonized standards are in development. Organizations should monitor official EU sources for updates and consult legal experts for compliance advice.
